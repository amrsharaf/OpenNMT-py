{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import torch.optim as optimizer\n",
    "from onmt.modules.discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Enviroment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Namespace(adapt=True, batch_size=64, brnn=False, brnn_merge='concat', curriculum=False, data='data/demo-train.pt', dropout=0.3, epochs=100, extra_shuffle=False, gpus=[1], input_feed=1, layers=2, learning_rate=.01, learning_rate_decay=0.5, log_interval=50, max_generator_batches=32, max_grad_norm=5, optim='sgd', param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc=None, rnn_size=500, save_model='model', start_decay_at=8, start_epoch=1, train_from='', train_from_state_dict='', word_vec_size=500)\n",
    "\n",
    "opt.cuda = len(opt.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup up the cuda Eviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.set_device(opt.gpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 'data/demo-train.pt'\n",
      " * vocabulary size. source = 24999; target = 35820\n",
      " * number of training sentences. 10000\n",
      " * maximum batch size. 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data from '%s'\" % opt.data)\n",
    "\n",
    "dataset = torch.load(opt.data)\n",
    "\n",
    "# type(dataset) = <type 'dict'>\n",
    "trainData = onmt.Dataset(dataset['train']['src'],\n",
    "                         dataset['train']['tgt'], opt.batch_size, opt.cuda)\n",
    "validData = onmt.Dataset(dataset['valid']['src'],\n",
    "                         dataset['valid']['tgt'], opt.batch_size, opt.cuda)\n",
    "\n",
    "domain_train = None\n",
    "domain_valid = None\n",
    "if opt.adapt:\n",
    "    assert('domain_train' in dataset)\n",
    "    assert('domain_valid' in dataset)\n",
    "    domain_train = onmt.Dataset(dataset['domain_train']['src'], None,\n",
    "                              opt.batch_size, opt.cuda)\n",
    "    domain_valid = onmt.Dataset(dataset['domain_valid']['src'], None,\n",
    "                              opt.batch_size, opt.cuda)\n",
    "\n",
    "\n",
    "dicts = dataset['dicts']\n",
    "print(' * vocabulary size. source = %d; target = %d' %\n",
    "      (dicts['src'].size(), dicts['tgt'].size()))\n",
    "print(' * number of training sentences. %d' %\n",
    "      len(dataset['train']['src']))\n",
    "print(' * maximum batch size. %d' % opt.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Len: 1\n",
      "--> Src Sentence: .\n",
      "--> Tgt Sentence: Aerobic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lookup_src(x):\n",
    "    return dicts['src'].idxToLabel[x]\n",
    "\n",
    "def lookup_tgt(x):\n",
    "    return dicts['tgt'].idxToLabel[x]\n",
    "\n",
    "for a in trainData.src:\n",
    "    a_list = a.numpy().tolist()\n",
    "    print \"--> Len: \" + str(len(a_list))\n",
    "    print \"--> Src Sentence: \" + str(\" \".join(map(lookup_src, a_list)))\n",
    "    print \"--> Tgt Sentence: \" + str(\" \".join(map(lookup_tgt, a_list))) + \"\\n\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "\n",
    "encoder = onmt.DomainModels.Encoder(opt, dicts['src'])\n",
    "decoder = onmt.DomainModels.Decoder(opt, dicts['tgt'])\n",
    "\n",
    "generator = nn.Sequential(\n",
    "        nn.Linear(opt.rnn_size, dicts['tgt'].size()),\n",
    "        nn.LogSoftmax())\n",
    "\n",
    "if opt.adapt:\n",
    "    discriminator = Discriminator(opt.word_vec_size  * opt.layers)\n",
    "model = onmt.DomainModels.NMTModel(encoder, decoder, discriminator)\n",
    "\n",
    "if len(opt.gpus) >= 1:\n",
    "    model.cuda()\n",
    "    generator.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "    generator.cpu()\n",
    "\n",
    "if len(opt.gpus) > 1:\n",
    "    model = nn.DataParallel(model, device_ids=opt.gpus, dim=1)\n",
    "    generator = nn.DataParallel(generator, device_ids=opt.gpus, dim=0)\n",
    "\n",
    "model.generator = generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoryEfficientLoss(outputs, targets, generator, crit, eval=False):\n",
    "    # compute generations one piece at a time\n",
    "    num_correct, loss = 0, 0\n",
    "    outputs = Variable(outputs.data, requires_grad=(not eval), volatile=eval)\n",
    "\n",
    "    batch_size = outputs.size(1)\n",
    "    outputs_split = torch.split(outputs, opt.max_generator_batches)\n",
    "    targets_split = torch.split(targets, opt.max_generator_batches)\n",
    "    for i, (out_t, targ_t) in enumerate(zip(outputs_split, targets_split)):\n",
    "        out_t = out_t.view(-1, out_t.size(2))\n",
    "        scores_t = generator(out_t)\n",
    "        loss_t = crit(scores_t, targ_t.view(-1))\n",
    "        pred_t = scores_t.max(1)[1]\n",
    "        num_correct_t = pred_t.data.eq(targ_t.data).masked_select(targ_t.ne(onmt.Constants.PAD).data).sum()\n",
    "        num_correct += num_correct_t\n",
    "        loss += loss_t.data[0]\n",
    "        if not eval:\n",
    "            loss_t.div(batch_size).backward()\n",
    "\n",
    "    grad_output = None if outputs.grad is None else outputs.grad.data\n",
    "    return loss, grad_output, num_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, criterion, data):\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    total_num_correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(len(data)):\n",
    "        batch = data[i][:-1] # exclude original indices\n",
    "        outputs = model(batch)\n",
    "        targets = batch[1][1:]  # exclude <s> from targets\n",
    "        loss, _, num_correct = memoryEfficientLoss(\n",
    "                outputs, targets, model.generator, criterion, eval=True)\n",
    "        total_loss += loss\n",
    "        total_num_correct += num_correct\n",
    "        total_words += targets.data.ne(onmt.Constants.PAD).sum()\n",
    "\n",
    "    model.train()\n",
    "    return float(total_loss) / float(total_words),\\\n",
    "           float(total_num_correct) / float(total_words)\n",
    "\n",
    "def domain_eval(model, data_old, data_new):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    total_num_discrim_correct, total_num_discrim_elements = 0, 0\n",
    "    for i in range(min(len(data_new),len(data_old))):\n",
    "        batch_old = data_old[i][:-1] # exclude original indices\n",
    "        batch_new = data_new[i][:-1]\n",
    "        \n",
    "        _, old_domain, new_domain = model(batch_old, domain_batch=batch_new)  \n",
    "        \n",
    "        tgts = Variable(torch.FloatTensor(len(old_domain) + len(new_domain),), requires_grad=False) \n",
    "            \n",
    "        if opt.cuda:\n",
    "            tgts = tgts.cuda()\n",
    "\n",
    "        tgts[:] = 0.0\n",
    "        tgts[:len(old_domain)] = 1.0\n",
    "        discrim_correct, num_discrim_elements = get_accuracy(torch.cat([old_domain, new_domain]).data.squeeze(), tgts.data)\n",
    "        \n",
    "        # Discriminator counts\n",
    "        total_num_discrim_correct += discrim_correct\n",
    "        total_num_discrim_elements += num_discrim_elements\n",
    "        \n",
    "    return float(total_num_discrim_correct) / float(total_num_discrim_elements)\n",
    "\n",
    "def get_accuracy(prediction, truth):\n",
    "    assert(prediction.nelement() == truth.nelement())\n",
    "    prediction[prediction < 0.5]  = 0.0\n",
    "    prediction[prediction >= 0.5] = 1.0\n",
    "    #accuracy = (100.0 * prediction.eq(truth).sum()) / float(prediction.nelement())\n",
    "    return prediction.eq(truth).sum(), float(prediction.nelement())\n",
    "    #return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMTCriterion(vocabSize):\n",
    "    weight = torch.ones(vocabSize)\n",
    "    weight[onmt.Constants.PAD] = 0\n",
    "    crit = nn.NLLLoss(weight, size_average=False)\n",
    "    if opt.cuda:\n",
    "        crit.cuda()\n",
    "    return crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences (batch,domain_batch):\n",
    "    for old_sentence_src, old_sentence_tgt, new_sentence in zip(batch[0],batch[1],domain_batch) :\n",
    "        old_sentence_src = [dataset['dicts']['src'].idxToLabel[x] for x in old_sentence_src.data]\n",
    "        old_sentence_src = \" \".join(old_sentence_src)\n",
    "        print \"old sentence src: \", old_sentence_src \n",
    "                    \n",
    "        old_sentence_tgt = [dataset['dicts']['tgt'].idxToLabel[x] for x in old_sentence_tgt.data]\n",
    "        old_sentence_tgt = \" \".join(old_sentence_tgt)\n",
    "        print \"old sentence tgt: \", old_sentence_tgt \n",
    "\n",
    "        new_sentence = [dataset['dicts']['src'].idxToLabel[x] for x in new_sentence.data]\n",
    "        new_sentence = \" \".join(new_sentence)\n",
    "        print \"\\nnew sentence: \", new_sentence\n",
    "                    \n",
    "        print \"-------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, trainData, validData, domain_train, domain_valid, dataset, optim):\n",
    "    print(model)\n",
    "    model.train()\n",
    "\n",
    "    # define criterion of each GPU\n",
    "    criterion = NMTCriterion(dataset['dicts']['tgt'].size())\n",
    "\n",
    "    start_time = time.time()\n",
    "    def trainEpoch(epoch):\n",
    "        \n",
    "        if opt.extra_shuffle and epoch > opt.curriculum:\n",
    "            trainData.shuffle()\n",
    "\n",
    "        # shuffle mini batch order\n",
    "        batchOrder = torch.randperm(len(trainData))\n",
    "        \n",
    "        discriminator_criterion = None\n",
    "        if opt.adapt:\n",
    "            batchOrderAdapt = torch.randperm(len(domain_train))\n",
    "            discriminator_criterion = nn.BCELoss()\n",
    "\n",
    "        total_num_discrim_correct, total_num_discrim_elements = 0, 0\n",
    "        total_loss, total_words, total_num_correct = 0, 0, 0\n",
    "        report_loss, report_tgt_words, report_src_words, report_num_correct = 0, 0, 0, 0\n",
    "        start = time.time()\n",
    "        for i in range(len(trainData)):\n",
    "\n",
    "            batchIdx = batchOrder[i] if epoch > opt.curriculum else i   \n",
    "            batch = trainData[batchIdx][:-1] # exclude original indices\n",
    "            \n",
    "            if debug:\n",
    "                print \"trainData batch size: \", trainData.batchSize\n",
    "                print \"batch len : \", len(batch)\n",
    "                \n",
    "                print \"\\n\\nbacth size: \", (len(batch[0][1]))\n",
    "                print \"other ---->: \", batch[0][1]\n",
    "                print \"batchIdx: \",batchIdx\n",
    "\n",
    "            model.zero_grad()\n",
    "            if opt.adapt:\n",
    "                batchIdxAdapt = batchOrderAdapt[i] if epoch >= opt.curriculum else i\n",
    "                batch_len = len(batch[0][1])\n",
    "                domain_batch = domain_train[batchIdxAdapt][:-1]\n",
    "                \n",
    "                if debug:\n",
    "                    print \"domain_train batch size: \", domain_train.batchSize\n",
    "                    print \"domain_batch[0] type: \", type(domain_batch[0])\n",
    "                    print \"domain_batch[0][0] type: \", type(domain_batch[0][0]), '\\n'\n",
    "\n",
    "                outputs, old_domain, new_domain = model(batch, domain_batch=domain_batch)       \n",
    "                discriminator_targets = Variable(torch.FloatTensor(len(old_domain) + len(new_domain),), requires_grad=False)\n",
    "\n",
    "                if opt.cuda:\n",
    "                    discriminator_targets = discriminator_targets.cuda()\n",
    "                \n",
    "                discriminator_targets[:] = 0.0\n",
    "                discriminator_targets[:len(old_domain)] = 1.0\n",
    "                discrim_correct, num_discrim_elements = get_accuracy(torch.cat([old_domain, new_domain]).data.squeeze(), discriminator_targets.data)\n",
    "                \n",
    "                discriminator_loss = discriminator_criterion(torch.cat([old_domain, new_domain]), discriminator_targets)\n",
    "            else:\n",
    "                outputs = model(batch)\n",
    "\n",
    "            targets = batch[1][1:]  # exclude <s> from targets\n",
    "            loss, gradOutput, num_correct = memoryEfficientLoss(\n",
    "                    outputs, targets, model.generator, criterion)\n",
    "\n",
    "            # We do the domain adaptation backward call here\n",
    "            if opt.adapt:\n",
    "                outputs.backward(gradOutput, retain_variables=True)\n",
    "                model.zero_grad()\n",
    "                discriminator_loss.backward()\n",
    "            else:\n",
    "                outputs.backward(gradOutput)\n",
    "    \n",
    "\n",
    "            # update the parameters\n",
    "            optim.step()\n",
    "\n",
    "            num_words = targets.data.ne(onmt.Constants.PAD).sum()\n",
    "            report_loss += loss\n",
    "            report_num_correct += num_correct\n",
    "            report_tgt_words += num_words\n",
    "            report_src_words += sum(batch[0][1])\n",
    "            total_loss += loss\n",
    "            total_num_correct += num_correct\n",
    "            total_words += num_words\n",
    "            \n",
    "            # Discriminator counts\n",
    "            total_num_discrim_correct += discrim_correct\n",
    "            total_num_discrim_elements += num_discrim_elements\n",
    "            \n",
    "            if i % opt.log_interval == -1 % opt.log_interval:\n",
    "                print(\"Epoch %2d, %5d/%5d; acc: %6.2f; ppl: %6.2f; %3.0f src tok/s; %3.0f tgt tok/s; %6.0f s elapsed\" %\n",
    "                      (epoch, i+1, len(trainData),\n",
    "                      report_num_correct / report_tgt_words * 100,\n",
    "                      math.exp(report_loss / report_tgt_words),\n",
    "                      report_src_words/(time.time()-start),\n",
    "                      report_tgt_words/(time.time()-start),\n",
    "                      time.time()-start_time))\n",
    "                \n",
    "                print \"discrim_correct: \", discrim_correct\n",
    "                print \"num_discrim_elements: \", num_discrim_elements, '\\n'\n",
    "\n",
    "                report_loss = report_tgt_words = report_src_words = report_num_correct = 0\n",
    "                start = time.time()\n",
    "\n",
    "        return float(total_loss) / float(total_words),\\\n",
    "               float(total_num_correct) / float(total_words),\\\n",
    "               float(total_num_discrim_correct) / float(total_num_discrim_elements)\n",
    "\n",
    "    for epoch in range(opt.start_epoch, opt.epochs + 1):\n",
    "        print('')\n",
    "\n",
    "        #  (1) train for one epoch on the training set\n",
    "        train_loss, train_acc, train_discrim_acc = trainEpoch(epoch)\n",
    "        print('Train perplexity: %g' % math.exp(min(train_loss, 100)))\n",
    "        print('Train accuracy: %g' % (train_acc*100))\n",
    "        print('Train discriminator accuracy: %g' % (train_discrim_acc * 100))\n",
    "\n",
    "        #  (2) evaluate on the validation set\n",
    "        valid_loss, valid_acc = eval(model, criterion, validData)\n",
    "        valid_discrim_acc = domain_eval(model, validData, domain_valid)\n",
    "        valid_ppl = math.exp(min(valid_loss, 100))\n",
    "        print('Validation perplexity: %g' % valid_ppl)\n",
    "        print('Validation accuracy: %g' % (valid_acc*100))\n",
    "        print('Validation discriminator accuracy: %g' % (valid_discrim_acc * 100))\n",
    "\n",
    "        \n",
    "        #  (3) update the learning rate\n",
    "        # optim.updateLearningRate(valid_loss, epoch)\n",
    "\n",
    "        model_state_dict = model.module.state_dict() if len(opt.gpus) > 1 else model.state_dict()\n",
    "        model_state_dict = {k: v for k, v in model_state_dict.items() if 'generator' not in k}\n",
    "        generator_state_dict = model.generator.module.state_dict() if len(opt.gpus) > 1 else model.generator.state_dict()\n",
    "        #  (4) drop a checkpoint\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            'generator': generator_state_dict,\n",
    "            'dicts': dataset['dicts'],\n",
    "            'opt': opt,\n",
    "            'epoch': epoch,\n",
    "            'optim': optim\n",
    "        }\n",
    "        torch.save(checkpoint,\n",
    "                   '%s_acc_%.2f_ppl_%.2f_e%d.pt' % (opt.save_model, 100*valid_acc, valid_ppl, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* number of parameters: 78133321\n",
      "NMTModel (\n",
      "  (encoder): Encoder (\n",
      "    (word_lut): Embedding(24999, 500, padding_idx=0)\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): Decoder (\n",
      "    (word_lut): Embedding(35820, 500, padding_idx=0)\n",
      "    (rnn): StackedLSTM (\n",
      "      (dropout): Dropout (p = 0.3)\n",
      "      (layers): ModuleList (\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention (\n",
      "      (linear_in): Linear (500 -> 500)\n",
      "      (sm): Softmax ()\n",
      "      (linear_out): Linear (1000 -> 500)\n",
      "      (tanh): Tanh ()\n",
      "    )\n",
      "    (dropout): Dropout (p = 0.3)\n",
      "  )\n",
      "  (discriminator): Discriminator (\n",
      "    (lin1): Linear (1000 -> 4000)\n",
      "    (drop): Dropout (p = 0.95)\n",
      "    (lin2): Linear (4000 -> 4000)\n",
      "    (lin3): Linear (4000 -> 1)\n",
      "  )\n",
      "  (generator): Sequential (\n",
      "    (0): Linear (500 -> 35820)\n",
      "    (1): LogSoftmax ()\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch  1,    50/  157; acc:   0.00; ppl: 36750.36; 4170 src tok/s; 4103 tgt tok/s;     18 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  1,   100/  157; acc:   0.00; ppl: 36777.57; 4238 src tok/s; 4189 tgt tok/s;     36 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  1,   150/  157; acc:   0.00; ppl: 36698.21; 3886 src tok/s; 3931 tgt tok/s;     53 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36739.2\n",
      "Train accuracy: 0.00535717\n",
      "Train discriminator accuracy: 49.8803\n",
      "Validation perplexity: 35464.5\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 49.4648\n",
      "\n",
      "Epoch  2,    50/  157; acc:   0.00; ppl: 36464.61; 4177 src tok/s; 4187 tgt tok/s;     79 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  2,   100/  157; acc:   0.00; ppl: 36463.72; 4394 src tok/s; 4384 tgt tok/s;     95 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  2,   150/  157; acc:   0.00; ppl: 36480.19; 4263 src tok/s; 4207 tgt tok/s;    112 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36471.3\n",
      "Train accuracy: 0.00223215\n",
      "Train discriminator accuracy: 50.1197\n",
      "Validation perplexity: 35467.5\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 50.5352\n",
      "\n",
      "Epoch  3,    50/  157; acc:   0.00; ppl: 36464.86; 4360 src tok/s; 4310 tgt tok/s;    138 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  3,   100/  157; acc:   0.00; ppl: 36501.90; 4409 src tok/s; 4334 tgt tok/s;    155 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  3,   150/  157; acc:   0.00; ppl: 36434.93; 3955 src tok/s; 4050 tgt tok/s;    171 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36474.4\n",
      "Train accuracy: 0.00178572\n",
      "Train discriminator accuracy: 49.8803\n",
      "Validation perplexity: 35468.7\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 49.4648\n",
      "\n",
      "Epoch  4,    50/  157; acc:   0.00; ppl: 36470.24; 4417 src tok/s; 4387 tgt tok/s;    197 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  4,   150/  157; acc:   0.00; ppl: 36561.26; 4417 src tok/s; 4352 tgt tok/s;    231 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36487.1\n",
      "Train accuracy: 0.00178572\n",
      "Train discriminator accuracy: 49.9352\n",
      "Validation perplexity: 35464.6\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 50.5352\n",
      "\n",
      "Epoch  5,    50/  157; acc:   0.00; ppl: 36510.43; 4189 src tok/s; 4191 tgt tok/s;    256 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  5,   100/  157; acc:   0.00; ppl: 36507.52; 4372 src tok/s; 4338 tgt tok/s;    273 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  5,   150/  157; acc:   0.00; ppl: 36482.24; 4215 src tok/s; 4211 tgt tok/s;    290 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36505.3\n",
      "Train accuracy: 0.00178572\n",
      "Train discriminator accuracy: 50.1496\n",
      "Validation perplexity: 35442.4\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 50.5352\n",
      "\n",
      "Epoch  6,    50/  157; acc:   0.00; ppl: 36580.70; 4301 src tok/s; 4274 tgt tok/s;    316 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  6,   100/  157; acc:   0.00; ppl: 36509.70; 4330 src tok/s; 4308 tgt tok/s;    333 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  6,   150/  157; acc:   0.00; ppl: 36546.70; 4177 src tok/s; 4159 tgt tok/s;    350 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36540.3\n",
      "Train accuracy: 0.00133929\n",
      "Train discriminator accuracy: 51.5363\n",
      "Validation perplexity: 35400.4\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 50.5352\n",
      "\n",
      "Epoch  7,    50/  157; acc:   0.00; ppl: 36703.43; 4539 src tok/s; 4419 tgt tok/s;    377 s elapsed\n",
      "discrim_correct:  69\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  7,   100/  157; acc:   0.00; ppl: 36477.24; 4154 src tok/s; 4174 tgt tok/s;    393 s elapsed\n",
      "discrim_correct:  13\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  7,   150/  157; acc:   0.00; ppl: 36514.13; 4164 src tok/s; 4194 tgt tok/s;    409 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36564.5\n",
      "Train accuracy: 0.000892861\n",
      "Train discriminator accuracy: 56.6191\n",
      "Validation perplexity: 35398.9\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 69.2753\n",
      "\n",
      "Epoch  8,    50/  157; acc:   0.00; ppl: 36514.12; 4381 src tok/s; 4370 tgt tok/s;    435 s elapsed\n",
      "discrim_correct:  111\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  8,   100/  157; acc:   0.00; ppl: 36496.52; 4168 src tok/s; 4208 tgt tok/s;    451 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  8,   150/  157; acc:   0.00; ppl: 36589.54; 4436 src tok/s; 4319 tgt tok/s;    469 s elapsed\n",
      "discrim_correct:  111\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36528.6\n",
      "Train accuracy: 0.000892861\n",
      "Train discriminator accuracy: 61.5323\n",
      "Validation perplexity: 35378.2\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 54.5183\n",
      "\n",
      "Epoch  9,    50/  157; acc:   0.00; ppl: 36575.12; 4566 src tok/s; 4519 tgt tok/s;    494 s elapsed\n",
      "discrim_correct:  106\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  9,   100/  157; acc:   0.00; ppl: 36434.83; 4119 src tok/s; 4181 tgt tok/s;    510 s elapsed\n",
      "discrim_correct:  74\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch  9,   150/  157; acc:   0.00; ppl: 36452.41; 4203 src tok/s; 4164 tgt tok/s;    527 s elapsed\n",
      "discrim_correct:  94\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36491.8\n",
      "Train accuracy: 0.000892861\n",
      "Train discriminator accuracy: 68.3874\n",
      "Validation perplexity: 35377.7\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 64.9763\n",
      "\n",
      "Epoch 10,    50/  157; acc:   0.00; ppl: 36395.74; 4273 src tok/s; 4253 tgt tok/s;    553 s elapsed\n",
      "discrim_correct:  64\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 10,   100/  157; acc:   0.00; ppl: 36475.96; 4498 src tok/s; 4461 tgt tok/s;    570 s elapsed\n",
      "discrim_correct:  99\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 10,   150/  157; acc:   0.00; ppl: 36431.50; 4115 src tok/s; 4116 tgt tok/s;    586 s elapsed\n",
      "discrim_correct:  61\n",
      "num_discrim_elements:  80.0 \n",
      "\n",
      "Train perplexity: 36435.2\n",
      "Train accuracy: 0.00178572\n",
      "Train discriminator accuracy: 72.1269\n",
      "Validation perplexity: 35401.1\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 63.4497\n",
      "\n",
      "Epoch 11,    50/  157; acc:   0.00; ppl: 36405.47; 4231 src tok/s; 4195 tgt tok/s;    613 s elapsed\n",
      "discrim_correct:  67\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 11,   100/  157; acc:   0.00; ppl: 36427.38; 4224 src tok/s; 4191 tgt tok/s;    630 s elapsed\n",
      "discrim_correct:  102\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 11,   150/  157; acc:   0.00; ppl: 36410.29; 4203 src tok/s; 4209 tgt tok/s;    646 s elapsed\n",
      "discrim_correct:  104\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36423.5\n",
      "Train accuracy: 0.000446431\n",
      "Train discriminator accuracy: 77.9579\n",
      "Validation perplexity: 35381.1\n",
      "Validation accuracy: 0\n",
      "Validation discriminator accuracy: 66.0291\n",
      "\n",
      "Epoch 12,    50/  157; acc:   0.00; ppl: 36470.89; 4219 src tok/s; 4230 tgt tok/s;    672 s elapsed\n",
      "discrim_correct:  78\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 12,   100/  157; acc:   0.00; ppl: 36556.15; 4788 src tok/s; 4719 tgt tok/s;    688 s elapsed\n",
      "discrim_correct:  124\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 12,   150/  157; acc:   0.00; ppl: 36516.18; 4513 src tok/s; 4491 tgt tok/s;    704 s elapsed\n",
      "discrim_correct:  120\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36508.3\n",
      "Train accuracy: 0.000446431\n",
      "Train discriminator accuracy: 83.764\n",
      "Validation perplexity: 35477.6\n",
      "Validation accuracy: 0.00624395\n",
      "Validation discriminator accuracy: 57.7294\n",
      "\n",
      "Epoch 13,    50/  157; acc:   0.00; ppl: 36521.91; 4571 src tok/s; 4564 tgt tok/s;    730 s elapsed\n",
      "discrim_correct:  115\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 13,   100/  157; acc:   0.00; ppl: 36492.70; 4496 src tok/s; 4532 tgt tok/s;    745 s elapsed\n",
      "discrim_correct:  122\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 13,   150/  157; acc:   0.00; ppl: 36554.79; 4941 src tok/s; 4873 tgt tok/s;    760 s elapsed\n",
      "discrim_correct:  109\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36529.8\n",
      "Train accuracy: 0.000892861\n",
      "Train discriminator accuracy: 86.9114\n",
      "Validation perplexity: 35475.5\n",
      "Validation accuracy: 0.00624395\n",
      "Validation discriminator accuracy: 60.4141\n",
      "\n",
      "Epoch 14,    50/  157; acc:   0.00; ppl: 36576.57; 4739 src tok/s; 4658 tgt tok/s;    788 s elapsed\n",
      "discrim_correct:  122\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 14,   100/  157; acc:   0.00; ppl: 36445.84; 4402 src tok/s; 4467 tgt tok/s;    802 s elapsed\n",
      "discrim_correct:  99\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 14,   150/  157; acc:   0.00; ppl: 36631.56; 4594 src tok/s; 4547 tgt tok/s;    819 s elapsed\n",
      "discrim_correct:  119\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36554.8\n",
      "Train accuracy: 0.000446431\n",
      "Train discriminator accuracy: 87.0112\n",
      "\n",
      "Epoch 15,    50/  157; acc:   0.00; ppl: 36520.81; 4519 src tok/s; 4533 tgt tok/s;    845 s elapsed\n",
      "discrim_correct:  114\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 15,   100/  157; acc:   0.00; ppl: 36489.30; 4569 src tok/s; 4569 tgt tok/s;    860 s elapsed\n",
      "discrim_correct:  106\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 15,   150/  157; acc:   0.00; ppl: 36684.97; 4880 src tok/s; 4791 tgt tok/s;    876 s elapsed\n",
      "discrim_correct:  112\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36568.5\n",
      "Train accuracy: 0.000892861\n",
      "Train discriminator accuracy: 88.4876\n",
      "Validation perplexity: 35538.2\n",
      "Validation accuracy: 0.00624395\n",
      "Validation discriminator accuracy: 58.9226\n",
      "\n",
      "Epoch 16,    50/  157; acc:   0.00; ppl: 36492.09; 4491 src tok/s; 4543 tgt tok/s;    901 s elapsed\n",
      "discrim_correct:  119\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 16,   100/  157; acc:   0.00; ppl: 36662.91; 4508 src tok/s; 4518 tgt tok/s;    917 s elapsed\n",
      "discrim_correct:  122\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 16,   150/  157; acc:   0.00; ppl: 36735.97; 5122 src tok/s; 5007 tgt tok/s;    933 s elapsed\n",
      "discrim_correct:  74\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36642.9\n",
      "Train accuracy: 0.000446431\n",
      "Train discriminator accuracy: 90.8719\n",
      "Validation perplexity: 35485.5\n",
      "Validation accuracy: 0.00468296\n",
      "Validation discriminator accuracy: 65.2922\n",
      "\n",
      "Epoch 17,    50/  157; acc:   0.00; ppl: 36630.27; 4504 src tok/s; 4459 tgt tok/s;    960 s elapsed\n",
      "discrim_correct:  121\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 17,   100/  157; acc:   0.00; ppl: 36693.24; 4715 src tok/s; 4699 tgt tok/s;    976 s elapsed\n",
      "discrim_correct:  125\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 17,   150/  157; acc:   0.00; ppl: 36608.48; 4510 src tok/s; 4564 tgt tok/s;    990 s elapsed\n",
      "discrim_correct:  119\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36655.4\n",
      "Train accuracy: 0.000446431\n",
      "Train discriminator accuracy: 91.8895\n",
      "Validation perplexity: 35527.8\n",
      "Validation accuracy: 0.00624395\n",
      "Validation discriminator accuracy: 63.4322\n",
      "\n",
      "Epoch 18,    50/  157; acc:   0.00; ppl: 36702.63; 4739 src tok/s; 4729 tgt tok/s;   1017 s elapsed\n",
      "discrim_correct:  126\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 18,   100/  157; acc:   0.00; ppl: 36639.68; 4764 src tok/s; 4724 tgt tok/s;   1033 s elapsed\n",
      "discrim_correct:  118\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 18,   150/  157; acc:   0.00; ppl: 36688.45; 4542 src tok/s; 4538 tgt tok/s;   1048 s elapsed\n",
      "discrim_correct:  119\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36675.1\n",
      "Train accuracy: 0.000446431\n",
      "Train discriminator accuracy: 91.7448\n",
      "Validation perplexity: 35567.6\n",
      "Validation accuracy: 0.00624395\n",
      "Validation discriminator accuracy: 59.7824\n",
      "\n",
      "Epoch 19,    50/  157; acc:   0.00; ppl: 36748.31; 4777 src tok/s; 4670 tgt tok/s;   1075 s elapsed\n",
      "discrim_correct:  122\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 19,   100/  157; acc:   0.00; ppl: 36649.34; 4437 src tok/s; 4530 tgt tok/s;   1088 s elapsed\n",
      "discrim_correct:  117\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Epoch 19,   150/  157; acc:   0.00; ppl: 36648.16; 4711 src tok/s; 4676 tgt tok/s;   1104 s elapsed\n",
      "discrim_correct:  126\n",
      "num_discrim_elements:  128.0 \n",
      "\n",
      "Train perplexity: 36688.1\n",
      "Train accuracy: 0.000446431\n",
      "Train discriminator accuracy: 91.266\n",
      "Validation perplexity: 35587.2\n",
      "Validation accuracy: 0.00624395\n",
      "Validation discriminator accuracy: 62.5022\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5728f356dac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'* number of parameters: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-7d70d9e83838>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, trainData, validData, domain_train, domain_valid, dataset, optim)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m#  (1) train for one epoch on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_discrim_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train perplexity: %g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train accuracy: %g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-7d70d9e83838>\u001b[0m in \u001b[0;36mtrainEpoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# We do the domain adaptation backward call here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kiante/workspace/venv/pytorch/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    p.data.uniform_(-opt.param_init, opt.param_init)\n",
    "\n",
    "encoder.load_pretrained_vectors(opt)\n",
    "decoder.load_pretrained_vectors(opt)\n",
    "\n",
    "optim = onmt.Optim(\n",
    "    opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
    "    lr_decay=opt.learning_rate_decay,\n",
    "    start_decay_at=opt.start_decay_at\n",
    ")\n",
    "\n",
    "# optim = optimizer.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "optim.set_parameters(model.parameters())\n",
    "\n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('* number of parameters: %d' % nParams)\n",
    "\n",
    "trainModel(model, trainData, validData, domain_train, domain_valid, dataset, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
